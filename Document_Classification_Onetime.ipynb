{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import copy\n",
    "import nltk\n",
    "import scipy\n",
    "import multiprocessing\n",
    "import operator, os, pickle\n",
    "import imp\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils as my_utils\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "       \n",
    "def preprocess(pd):\n",
    "    pd = pd.str.lower()\n",
    "    pd = pd.str.replace('[^a-zA-Z]', ' ')\n",
    "    pd = pd.apply(lambda x: [w for w in w_tokenizer.tokenize(str(x))])\n",
    "    pd = pd.str.join(' ')\n",
    "    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])    \n",
    "    pd = pd.apply(lambda x: [item for item in x if len(item)>1])\n",
    "    return pd\n",
    "\n",
    "def process_df_body(df):\n",
    "    df['body_text'] = preprocess(df['body']).apply(lambda x: \" \".join(x))\n",
    "    return df\n",
    "\n",
    "def process_df_title(df):\n",
    "    df['title_text'] = preprocess(df['title']).apply(lambda x: \" \".join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../nontracked/\"\n",
    "dataset_name = \"dataset_cleaned_document_classification\"\n",
    "csv_name = \"items_old.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found Cache File, Loading...\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(path + dataset_name):\n",
    "    print(\"\\nFound Cache File, Loading...\")\n",
    "    dataset = pd.read_pickle(path + dataset_name)\n",
    "else:\n",
    "    print(\"\\nCache Not Found, Generating Cache...\")\n",
    "    \n",
    "    dataset = pd.read_csv(path + csv_name, header=None)\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.rename(columns={1:'title', 2:'body'})\n",
    "\n",
    "    dataset['body'] = dataset['body'].astype(str)\n",
    "    dataset['title'] = dataset['title'].astype(str)\n",
    "\n",
    "    n_cores = 45\n",
    "\n",
    "    n = int(dataset.shape[0]/n_cores)\n",
    "\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    list_df = [dataset[i:i+n] for i in range(0, dataset.shape[0], n)]\n",
    "\n",
    "    print(\"Processing title...\")\n",
    "    processed_list_df = pool.map(process_df_title, list_df)\n",
    "    pool.close()\n",
    "    dataset = pd.concat(processed_list_df)\n",
    "\n",
    "    print(\"Processing body...\")\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    list_df = [dataset[i:i+n] for i in range(0, dataset.shape[0],n)]\n",
    "\n",
    "    processed_list_df = pool.map(process_df_body, list_df)\n",
    "    pool.close()\n",
    "    dataset = pd.concat(processed_list_df)\n",
    "\n",
    "    dataset['text'] = dataset[['title_text', 'body_text']].agg(' '.join, axis=1).astype(str)\n",
    "\n",
    "    dataset.to_pickle(path + dataset_name)\n",
    "    print(\"Cache dumped...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "advt = dataset[['title', 'body']].agg(' '.join, axis=1).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1 = advt.apply(lambda x: True if (('amazon deal' in x) or ('$' in x and 'amazon' in x) or ('price' in x and 'drop' in x) or ('deals' in x) or ('limited' in x and 'time' in x)) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "advt_ = dataset[a_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "advt_['y'] = 'advt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stackexchange.com\n",
    "# stackoverflow.com\n",
    "# superuser.com\n",
    "# serverfault.com\n",
    "# community.byte.co\n",
    "# discuss.linuxcontainers.org\n",
    "# forum.lazarus.freepascal.org\n",
    "# forums.lutris.net\n",
    "# forum.asrock.com\n",
    "# answers.yahoo.com\n",
    "# answers.com\n",
    "# techcommunity.microsoft.com\n",
    "# uberpeople.net\n",
    "# quora.com\n",
    "# forums.macrumors.com\n",
    "# community.shopify.com\n",
    "# forums.wyzecam.com\n",
    "# community.spotify.com\n",
    "# forums.whonix.org\n",
    "# forum.opnsense.org\n",
    "# forum.odroid.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "forum = dataset[dataset[3].apply(lambda x: True if ('stackexchange.com' in x or 'stackoverflow.com' in x or 'superuser.com' in x or 'serverfault.com' in x or 'community.byte.co' in x or 'discuss.linuxcontainers.org' in x or 'forum.lazarus.freepascal.org' in x or 'forums.lutris.net' in x or 'forum.asrock.com' in x or 'answers.yahoo.com' in x or 'answers.com' in x or 'techcommunity.microsoft.com' in x or 'uberpeople.net' in x or 'quora.com' in x or 'forums.macrumors.com' in x or 'community.shopify.com' in x or 'forums.wyzecam.com' in x or 'community.spotify.com' in x or 'forums.whonix.org' in x or 'forum.opnsense.org' in x or 'forum.odroid.com' in x) else False)]\n",
    "forum['y'] = 'forum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107901, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arstechnica.com\n",
    "# blogs.nvidia.com\n",
    "# variety.com\n",
    "# businessinsider.com\n",
    "# spacedaily.com\n",
    "# theguardian.com\n",
    "# interestingengineering.com\n",
    "# nysenasdaqlive.com\n",
    "# seekingalpha.com\n",
    "# finance.yahoo.com\n",
    "# aithority.com\n",
    "# techcrunch.com\n",
    "# bloombergquint.com\n",
    "# theverge.com\n",
    "# aljazeera.com\n",
    "# cnbc.com\n",
    "# vox.com\n",
    "# nme.com\n",
    "# nytimes.com\n",
    "# seattletimes.com\n",
    "# telegraph.co.uk\n",
    "# ben-evans.com\n",
    "# thedrum.com\n",
    "# ft.com\n",
    "# cnn.com\n",
    "# bloomberg.com\n",
    "# reuters.com\n",
    "# ibtimes.com\n",
    "# apnews.com\n",
    "# ibtimes.com\n",
    "# usatoday.com\n",
    "# hackernoon.com\n",
    "# thenextweb.com\n",
    "# venturebeat.com\n",
    "# informationweek.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "good = dataset[dataset[3].apply(lambda x: True if ('arstechnica.com' in x or 'blogs.nvidia.com' in x or 'variety.com' in x or 'businessinsider.com' in x or 'spacedaily.com' in x or 'theguardian.com' in x or 'interestingengineering.com' in x or 'nysenasdaqlive.com' in x or 'seekingalpha.com' in x or 'finance.yahoo.com' in x or 'aithority.com' in x or 'techcrunch.com' in x or 'bloombergquint.com' in x or 'theverge.com' in x or 'aljazeera.com' in x or 'cnbc.com' in x or 'vox.com' in x or 'nme.com' in x or 'nytimes.com' in x or 'seattletimes.com' in x or 'telegraph.co.uk' in x or 'theverge.com' in x or 'thedrum.com' in x or 'ft.com' in x or 'cnn.com' in x or 'bloomberg.com' in x or 'reuters.com' in x or 'ibtimes.com' in x or 'apnews.com' in x or 'venturebeat.com' in x or 'usatoday.com' in x or 'hackernoon.com' in x or 'thenextweb.com' in x or 'informationweek.com' in x) else False)]\n",
    "good['y'] = 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((129944, 10), (107901, 10), (73905, 10))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advt_.shape, forum.shape, good.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pd.concat([advt_, forum, good])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = subset.groupby('y')\n",
    "subset = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'advt': 73905, 'forum': 73905, 'good': 73905})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(subset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "# Numbers\n",
    "# Alphabets\n",
    "# Special Characters\n",
    "# Count of uppercase\n",
    "# Count of lowercase\n",
    "# Number of words\n",
    "# total character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_tr, subset_te = train_test_split(subset, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "subset_tr['raw_text'] = subset_tr[['title', 'body']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "subset_te['raw_text'] = subset_te[['title', 'body']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = set(pd.read_csv(\"ner_list\", header=None, sep=\"\\\\\").values.reshape(-1))\n",
    "ner = {i:idx for idx, i in enumerate(ner)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(text):\n",
    "    pos = nltk.pos_tag(word_tokenize(text))\n",
    "\n",
    "    m = {i:0 for i in [j[1] for j in pos]}\n",
    "    m.keys\n",
    "    for _, j in pos:\n",
    "        m[j] += 1\n",
    "    n = [0] * len(ner)\n",
    "    for k, v in m.items():\n",
    "        n[ner[k]] = v\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_count(inp):\n",
    "    text, raw_text = inp\n",
    "    v_raw_tl = len(raw_text)\n",
    "    v_proc_tl = len(text)\n",
    "    v_raw_wc = len(word_tokenize(raw_text))\n",
    "    v_proc_wc = len(word_tokenize(text))\n",
    "    v_num = len(re.findall(r'([\\d])', raw_text))\n",
    "    v_uppercase = len(re.findall(r'([A-Z])', raw_text))\n",
    "    v_lower = len(re.findall(r'([a-z])', raw_text))\n",
    "    v_spl_chrs = len(re.findall(r'([^(a-zA-Z\\d \\n)])', raw_text))\n",
    "    v_new_line = len(re.findall(r'([\\n])', raw_text))\n",
    "    return [v_raw_tl, v_proc_tl, v_raw_wc, v_proc_wc, v_num, v_uppercase, v_lower, v_spl_chrs, v_new_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 18.4 s, total: 20.6 s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "ner_list = pool.map(get_pos_tags, subset_tr['raw_text'].tolist())\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 31.5 s, total: 32.5 s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "te_ner_list = pool.map(get_pos_tags, subset_te['raw_text'].tolist())\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(n_cores)\n",
    "special_counts = pool.map(get_special_count, subset_tr[['text', 'raw_text']].values.tolist())\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(n_cores)\n",
    "te_special_counts = pool.map(get_special_count, subset_te[['text', 'raw_text']].values.tolist())\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 20\n",
    "max_df = .6\n",
    "max_features = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                             stop_words=\"english\", max_features=max_features,\n",
    "                             max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = vectorizer.fit_transform(subset_tr['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vectorizer.transform(subset_te['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40387"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([ner_list, special_counts, train_vec.toarray()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate([te_ner_list, te_special_counts, test_vec.toarray()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "                       oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "                       oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, subset_tr['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_new(text):\n",
    "    text = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(re.sub(r'[^a-zA-Z]', \" \", text.lower()))]\n",
    "    return \" \".join([i for i in text if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = subset_tr.iloc[0]['raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessing_new(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_raw_pos = get_pos_tags(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_spl_count = get_special_count([text, raw_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_vec = vectorizer.transform([text]).toarray()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = te_raw_pos + te_spl_count + te_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = {}\n",
    "for i, j in zip(clf.classes_, clf.predict_proba([X_te])[0]):\n",
    "    ret[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8630835149966173"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, subset_te['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, \"vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, \"clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pos_name(j):\n",
    "#     return [i[1] for i in nltk.pos_tag(word_tokenize(j))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# ner_list = pool.map(get_pos_name, subset_tr['raw_text'].tolist())\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner = set()\n",
    "# for i in tqdm(raw_text_results):\n",
    "#     ner |= set(i.keys())\n",
    "# pd.DataFrame([ner]).T.to_csv(\"ner_list\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     filenames = glob.glob(path + \"items_old.csv\")\n",
    "\n",
    "#     dataset = []\n",
    "#     for filename in filenames:\n",
    "#         dataset.append(pd.read_csv(filename, header=None))\n",
    "#     dataset = pd.concat(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
